{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeanReverter Strategy Batch Optimization\n",
    "\n",
    "这个notebook用于对多个交易对进行MeanReverter策略的批量优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库...\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import backtrader as bt \n",
    "import optuna\n",
    "import warnings\n",
    "import quantstats as qs\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path  # 使用pathlib代替os\n",
    "# 添加必要的导入\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# _data_loading_executor = ThreadPoolExecutor(max_workers=10)  # 用于数据加载的线程池\n",
    "\n",
    "# 遍历每个交易对，但使用多进程方式\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "# 在此处添加全局缓存字典，用于缓存数据加载结果和数据完整性检查结果\n",
    "_data_feed_cache = {}\n",
    "_data_completeness_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeanReverter import MeanReverter\n",
    "\n",
    "# 在 CONFIG 中添加所有需要动态配置的参数\n",
    "CONFIG = {\n",
    "    # 策略相关配置\n",
    "    'strategy': {\n",
    "        'class': MeanReverter,\n",
    "        'name': MeanReverter.__name__\n",
    "    },\n",
    "    \n",
    "    # 数据相关配置（单币种、单时间周期）\n",
    "    # 如果 selected_symbols 为空，则通过 get_all_symbols 自动获取所有交易对\n",
    "    'selected_symbols': [],\n",
    "    'data_path': r'..\\\\futures',\n",
    "    'start_date': '2024-01-01',\n",
    "    'end_date': '2025-02-08',\n",
    "    'source_timeframe': '1m',\n",
    "    # 针对批量优化使用多个目标时间周期\n",
    "    'target_timeframes': ['15min'],\n",
    "    \n",
    "    # 文件保存配置\n",
    "    'reports_path': 'reports',\n",
    "    'results_filename_template': 'optimization_results_{strategy_name}_{start_date}-{end_date}.csv',\n",
    "    \n",
    "    # 回测参数配置\n",
    "    'commission': 0.0004,\n",
    "    'initial_capital': 10000,\n",
    "    # 如果需要可以添加：\n",
    "    # 'trade_on_close': True,\n",
    "    # 'exclusive_orders': True,\n",
    "    # 'hedging': False,\n",
    "    \n",
    "    # 优化参数配置，根据 MeanReverter 策略的参数进行优化\n",
    "    'optimization_params': {\n",
    "        'frequency': range(15, 31, 2),            # 用于计算慢速 RSI 均线的周期，步长为2\n",
    "        'rsiFrequency': range(30, 46, 2),         # 计算 RSI 的周期，步长为2\n",
    "        'buyZoneDistance': range(1, 8, 1),        # RSI 相对于慢速 RSI 均线的折扣比例，步长为1\n",
    "        'avgDownATRSum': range(3, 8, 1),          # 用于计算 ATR 累积值的周期数，步长为1\n",
    "        'useAbsoluteRSIBarrier': [True, False],   # 是否使用绝对 RSI 阈值进行平仓\n",
    "        'barrierLevel': range(55, 66, 2),         # RSI 阻力水平，步长为2\n",
    "        'pyramiding': range(2, 5, 1)              # 最大允许加仓次数，步长为1\n",
    "    },\n",
    "    \n",
    "    # 优化设置\n",
    "    'optimization_settings': {\n",
    "        'n_trials': 240,       # 可根据需要调整试验次数\n",
    "        'min_trades': 50,\n",
    "        'timeout': 3600,\n",
    "        'n_jobs': 60           # -1 表示使用所有 CPU 核心; 也可以设置为具体的数量\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeframe_params(timeframe_str):\n",
    "    \"\"\"\n",
    "    将时间周期字符串转换为 backtrader 的 timeframe 和 compression 参数\n",
    "    \"\"\"\n",
    "    if timeframe_str.endswith('min'):\n",
    "        return (bt.TimeFrame.Minutes, int(timeframe_str.replace('min', '')))\n",
    "    elif timeframe_str.endswith('H'):\n",
    "        return (bt.TimeFrame.Minutes, int(timeframe_str.replace('H', '')) * 60)\n",
    "    elif timeframe_str.endswith('D'):\n",
    "        return (bt.TimeFrame.Days, 1)\n",
    "    elif timeframe_str == '1m':\n",
    "        return (bt.TimeFrame.Minutes, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的时间周期格式: {timeframe_str}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_and_resample_data(symbol, start_date, end_date, source_timeframe='1m', target_timeframe='30min', data_path=r'..\\\\futures'):\n",
    "    \"\"\"\n",
    "    加载并重采样期货数据，并缓存已经重采样后的 DataFrame 以避免重复 I/O 操作\n",
    "    \"\"\"\n",
    "    # 构造缓存键\n",
    "    key = (symbol, start_date, end_date, source_timeframe, target_timeframe, data_path)\n",
    "    if key in _data_feed_cache:\n",
    "        # 如果缓存中有，返回新的数据馈送对象（注意拷贝，防止被修改）\n",
    "        cached_df = _data_feed_cache[key]\n",
    "        timeframe, compression = get_timeframe_params(target_timeframe)\n",
    "        data_feed = bt.feeds.PandasData(\n",
    "            dataname=cached_df.copy(),\n",
    "            open='Open',\n",
    "            high='High',\n",
    "            low='Low',\n",
    "            close='Close',\n",
    "            volume='Volume',\n",
    "            openinterest=-1,\n",
    "            timeframe=timeframe,\n",
    "            compression=compression,\n",
    "            fromdate=pd.to_datetime(start_date),\n",
    "            todate=pd.to_datetime(end_date)\n",
    "        )\n",
    "        \n",
    "        # 添加clone方法，这样可以快速创建数据副本而不需要重新执行IO\n",
    "        data_feed.clone = lambda: bt.feeds.PandasData(\n",
    "            dataname=cached_df.copy(),\n",
    "            open='Open',\n",
    "            high='High',\n",
    "            low='Low',\n",
    "            close='Close',\n",
    "            volume='Volume',\n",
    "            openinterest=-1,\n",
    "            timeframe=timeframe,\n",
    "            compression=compression,\n",
    "            fromdate=pd.to_datetime(start_date),\n",
    "            todate=pd.to_datetime(end_date)\n",
    "        )\n",
    "        \n",
    "        return data_feed\n",
    "    \n",
    "    # 生成日期范围\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    all_data = []\n",
    "    \n",
    "    # 标准化交易对名称\n",
    "    formatted_symbol = symbol.replace('/', '_').replace(':', '_')\n",
    "    if not formatted_symbol.endswith('USDT'):\n",
    "        formatted_symbol = f\"{formatted_symbol}USDT\"\n",
    "    \n",
    "    # 顺序读取文件，不使用线程池\n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        # 构建文件路径\n",
    "        file_path = os.path.join(data_path, date_str, f\"{date_str}_{formatted_symbol}_USDT_{source_timeframe}.csv\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                # 读取数据\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "                all_data.append(df)\n",
    "            else:\n",
    "                print(f\"文件不存在: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件出错 {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(f\"未找到 {symbol} 在指定日期范围内的数据\")\n",
    "    \n",
    "    # 合并、排序，以及重采样数据\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('datetime')\n",
    "    combined_df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    resampled = combined_df.resample(target_timeframe).agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).dropna()  # 立即删除NaN值\n",
    "    \n",
    "    backtesting_df = pd.DataFrame({\n",
    "        'Open': resampled['open'],\n",
    "        'High': resampled['high'],\n",
    "        'Low': resampled['low'],\n",
    "        'Close': resampled['close'],\n",
    "        'Volume': resampled['volume']\n",
    "    })\n",
    "    \n",
    "    # 确保所有数据都是数值类型并删除任何无效值\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        backtesting_df[col] = pd.to_numeric(backtesting_df[col], errors='coerce')\n",
    "    backtesting_df = backtesting_df.dropna()\n",
    "    \n",
    "    # 将结果缓存在全局变量中（使用拷贝，以免后续被修改）\n",
    "    _data_feed_cache[key] = backtesting_df.copy()\n",
    "    \n",
    "    timeframe, compression = get_timeframe_params(target_timeframe)\n",
    "    data_feed = bt.feeds.PandasData(\n",
    "        dataname=backtesting_df,\n",
    "        open='Open',\n",
    "        high='High',\n",
    "        low='Low',\n",
    "        close='Close',\n",
    "        volume='Volume',\n",
    "        openinterest=-1,\n",
    "        timeframe=timeframe,\n",
    "        compression=compression,\n",
    "        fromdate=pd.to_datetime(start_date),\n",
    "        todate=pd.to_datetime(end_date)\n",
    "    )\n",
    "    \n",
    "    # 添加clone方法\n",
    "    data_feed.clone = lambda: bt.feeds.PandasData(\n",
    "        dataname=backtesting_df.copy(),\n",
    "        open='Open',\n",
    "        high='High',\n",
    "        low='Low',\n",
    "        close='Close',\n",
    "        volume='Volume',\n",
    "        openinterest=-1,\n",
    "        timeframe=timeframe,\n",
    "        compression=compression,\n",
    "        fromdate=pd.to_datetime(start_date),\n",
    "        todate=pd.to_datetime(end_date)\n",
    "    )\n",
    "    \n",
    "    return data_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_symbols(data_path, date_str):\n",
    "    \"\"\"获取指定日期目录下的所有交易对\"\"\"\n",
    "    daily_path = os.path.join(data_path, date_str)\n",
    "    if not os.path.exists(daily_path):\n",
    "        return []\n",
    "    \n",
    "    files = glob(os.path.join(daily_path, f\"{date_str}_*_USDT_1m.csv\"))\n",
    "    symbols = set()  # 使用 set 进行去重\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)\n",
    "        symbol = filename.split('_')[1]\n",
    "        symbols.add(symbol)\n",
    "    return list(symbols)\n",
    "\n",
    "def verify_data_completeness(symbol, start_date, end_date, data_path):\n",
    "    \"\"\"验证数据完整性\"\"\"\n",
    "    # 构造缓存键\n",
    "    key = (symbol, start_date, end_date, data_path)\n",
    "    if key in _data_completeness_cache:\n",
    "        return _data_completeness_cache[key]\n",
    "    \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # 标准化交易对名称\n",
    "    formatted_symbol = symbol.replace('/', '_').replace(':', '_')\n",
    "    if not formatted_symbol.endswith('USDT'):\n",
    "        formatted_symbol = f\"{formatted_symbol}USDT\"\n",
    "    \n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        file_path = os.path.join(\n",
    "            data_path,\n",
    "            date_str,\n",
    "            f\"{date_str}_{formatted_symbol}_USDT_1m.csv\"  # 文件名格式保持不变\n",
    "        )\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            _data_completeness_cache[key] = False\n",
    "            return False\n",
    "    _data_completeness_cache[key] = True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加自定义评分函数\n",
    "def custom_score(strat):\n",
    "    \"\"\"\n",
    "    自定义评分函数 - 以最大化回报率为主要目标\n",
    "    保留最低交易次数要求作为基本约束\n",
    "    \"\"\"\n",
    "    # 获取交易次数\n",
    "    trades = strat.analyzers.trades.get_analysis()\n",
    "    total_trades = trades.get('total', {}).get('total', 0)\n",
    "    \n",
    "    # 从returns分析器获取总回报率\n",
    "    returns = strat.analyzers.returns.get_analysis()\n",
    "    total_return = returns.get('rtot', 0) * 100  # 转为百分比\n",
    "    \n",
    "    # 交易次数惩罚 - 确保策略至少有足够的交易\n",
    "    min_trades = CONFIG['optimization_settings'].get('min_trades', 50)\n",
    "    trade_penalty = 1.0 if total_trades >= min_trades else (total_trades / min_trades) ** 2\n",
    "    \n",
    "    # 最终得分简单地使用调整后的回报率\n",
    "    # 交易次数不足的策略会受到严厉惩罚\n",
    "    score = total_return * trade_penalty\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_strategy(symbol, timeframe):\n",
    "    \"\"\"\n",
    "    使用 Optuna 优化策略参数，并返回最优的前 5 个参数组合\n",
    "    \"\"\"\n",
    "    # 1. 预加载数据，只执行一次IO操作\n",
    "    preloaded_data = load_and_resample_data(\n",
    "        symbol, CONFIG['start_date'], CONFIG['end_date'],\n",
    "        target_timeframe=timeframe\n",
    "    )\n",
    "    \n",
    "    # 使用内存存储而非SQLite数据库\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"{symbol}_{timeframe}\",\n",
    "        direction=\"maximize\",\n",
    "        storage=None  # 使用内存存储\n",
    "    )\n",
    "    \n",
    "    print(f\"开始优化 {symbol}-{timeframe}...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        try:\n",
    "            params = {}\n",
    "            for param_name, param_range in CONFIG['optimization_params'].items():\n",
    "                if isinstance(param_range, range):\n",
    "                    params[param_name] = trial.suggest_int(\n",
    "                        param_name,\n",
    "                        param_range.start,\n",
    "                        param_range.stop - 1,\n",
    "                        param_range.step\n",
    "                    )\n",
    "                else:\n",
    "                    params[param_name] = trial.suggest_categorical(param_name, param_range)\n",
    "            \n",
    "            cerebro = bt.Cerebro(\n",
    "                        optdatas=True,    # 启用数据优化\n",
    "                        optreturn=True,   # 仅返回必要结果\n",
    "                        runonce=True,     # 批处理模式\n",
    "                        preload=True      # 预加载数据\n",
    "            )\n",
    "            # 2. 使用预加载数据的克隆而不是重新加载数据\n",
    "            data = preloaded_data.clone()\n",
    "            cerebro.adddata(data)\n",
    "            cerebro.addstrategy(CONFIG['strategy']['class'], **params)\n",
    "            \n",
    "            cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\n",
    "            cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n",
    "            cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')\n",
    "            cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')\n",
    "            cerebro.addanalyzer(bt.analyzers.SQN, _name='sqn')\n",
    "            \n",
    "            results = cerebro.run()\n",
    "            strat = results[0]\n",
    "            score = custom_score(strat)\n",
    "            \n",
    "            # 每个Trial都会输出一行信息\n",
    "            print(f\"[{symbol}-{timeframe}] Trial {trial.number}: 参数 {params} -> 得分 {score:.2f}\")\n",
    "            \n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}-{timeframe}] Trial {trial.number} 出错: {e}\")\n",
    "            # 返回极低的分数，确保该试验不会被选中\n",
    "            return float('-inf')\n",
    "    \n",
    "    # 在 study.optimize 里并行执行多个 Trial\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=CONFIG['optimization_settings']['n_trials'],\n",
    "        timeout=CONFIG['optimization_settings']['timeout'],\n",
    "        n_jobs=CONFIG['optimization_settings'].get('n_jobs', 1),\n",
    "        catch=(Exception,)  # 捕获所有异常\n",
    "    )\n",
    "    \n",
    "    print(f\"完成 {symbol}-{timeframe} 的优化\")\n",
    "    \n",
    "    # 提取前 5 个最佳试验\n",
    "    completed_trials = [\n",
    "        t for t in study.trials\n",
    "        if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None and t.value > float('-inf')\n",
    "    ]\n",
    "    \n",
    "    if not completed_trials:\n",
    "        print(f\"警告: {symbol}-{timeframe} 没有有效的完成试验\")\n",
    "        return []\n",
    "    \n",
    "    top_trials = sorted(completed_trials, key=lambda t: t.value, reverse=True)[:5]\n",
    "    \n",
    "    print(f\"[{symbol}-{timeframe}] 最佳参数组合:\")\n",
    "    top_results = []\n",
    "    for i, t in enumerate(top_trials, 1):\n",
    "        result = t.params.copy()\n",
    "        result['score'] = t.value\n",
    "        top_results.append(result)\n",
    "        print(f\"  Rank {i}: 得分 {t.value:.2f}, 参数: {t.params}\")\n",
    "    \n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest_with_params(params, symbol, timeframe):\n",
    "    \"\"\"\n",
    "    使用指定参数运行策略的回测并计算收益指标（利用 quantstats）。\n",
    "    返回一个包含基础回测指标和所有 quantstats 指标的字典。\n",
    "    \"\"\"\n",
    "    # 过滤掉不属于策略参数部分的键（如 'score', 'symbol', 'timeframe'等）\n",
    "    valid_keys = set(CONFIG[\"optimization_params\"].keys())\n",
    "    strategy_params = {k: v for k, v in params.items() if k in valid_keys}\n",
    "\n",
    "    cerebro = bt.Cerebro(\n",
    "                optdatas=True,    # 启用数据优化\n",
    "                optreturn=True,   # 仅返回必要结果\n",
    "                runonce=True,     # 批处理模式\n",
    "                preload=True      # 预加载数据\n",
    "    )\n",
    "    data = load_and_resample_data(symbol, CONFIG['start_date'], CONFIG['end_date'],\n",
    "                                  target_timeframe=timeframe)\n",
    "    cerebro.adddata(data)\n",
    "    # 只传入过滤后的策略参数\n",
    "    cerebro.addstrategy(CONFIG['strategy']['class'], **strategy_params)\n",
    "\n",
    "    initial_capital = CONFIG['initial_capital']\n",
    "    cerebro.broker.setcash(initial_capital)\n",
    "    cerebro.broker.setcommission(commission=CONFIG['commission'])\n",
    "\n",
    "    # 添加常用分析器，包括 PyFolio 用于后续量化指标计算\n",
    "    # cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\n",
    "    # cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n",
    "    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')\n",
    "    cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')\n",
    "    cerebro.addanalyzer(bt.analyzers.PyFolio, _name='pyfolio')\n",
    "\n",
    "    results = cerebro.run()\n",
    "    strat = results[0]\n",
    "    final_value = cerebro.broker.getvalue()\n",
    "    profit = final_value - initial_capital\n",
    "    roi = (profit / initial_capital) * 100\n",
    "\n",
    "    # 获取 PyFolio 的回测收益率数据\n",
    "    portfolio_stats = strat.analyzers.pyfolio.get_pf_items()\n",
    "    returns = portfolio_stats[0]\n",
    "    \n",
    "    # 修改这里：确保索引没有时区信息，避免使用tz_convert\n",
    "    # 首先检查是否有时区信息再进行处理\n",
    "    try:\n",
    "        if hasattr(returns.index, 'tz') and returns.index.tz is not None:\n",
    "            returns.index = returns.index.tz_localize(None)\n",
    "    except:\n",
    "        # 如果无法处理时区，创建一个新的无时区索引\n",
    "        try:\n",
    "            returns = pd.Series(returns.values, index=pd.DatetimeIndex(returns.index.astype('datetime64[ns]')))\n",
    "        except:\n",
    "            # 如果依然失败，使用更简单的方法\n",
    "            returns = pd.Series(returns.values, index=pd.DatetimeIndex([str(idx) for idx in returns.index]))\n",
    "\n",
    "    # 计算量化指标（完整的收益指标）\n",
    "    qs_stats = {}\n",
    "    try:\n",
    "        qs_stats[\"Sharpe Ratio\"] = qs.stats.sharpe(returns)\n",
    "        qs_stats[\"Sortino Ratio\"] = qs.stats.sortino(returns)\n",
    "        qs_stats[\"Calmar Ratio\"] = qs.stats.calmar(returns)\n",
    "        qs_stats[\"Max Drawdown\"] = qs.stats.max_drawdown(returns)\n",
    "        qs_stats[\"Win Rate\"] = qs.stats.win_rate(returns)\n",
    "        qs_stats[\"Profit Factor\"] = qs.stats.profit_factor(returns)\n",
    "        qs_stats[\"Expected Return (M)\"] = qs.stats.expected_return(returns, aggregate='M')\n",
    "        qs_stats[\"Kelly Criterion\"] = qs.stats.kelly_criterion(returns)\n",
    "        qs_stats[\"Risk of Ruin\"] = qs.stats.risk_of_ruin(returns)\n",
    "        qs_stats[\"Tail Ratio\"] = qs.stats.tail_ratio(returns)\n",
    "        qs_stats[\"Common Sense Ratio\"] = qs.stats.common_sense_ratio(returns)\n",
    "        qs_stats[\"Average Win\"] = qs.stats.avg_win(returns)\n",
    "        qs_stats[\"Average Loss\"] = qs.stats.avg_loss(returns)\n",
    "        qs_stats[\"Annualized Volatility\"] = qs.stats.volatility(returns, periods=252)\n",
    "        qs_stats[\"Skew\"] = qs.stats.skew(returns)\n",
    "        qs_stats[\"Kurtosis\"] = qs.stats.kurtosis(returns)\n",
    "        qs_stats[\"Value at Risk\"] = qs.stats.value_at_risk(returns)\n",
    "        qs_stats[\"Conditional VaR\"] = qs.stats.conditional_value_at_risk(returns)\n",
    "        qs_stats[\"Payoff Ratio\"] = qs.stats.payoff_ratio(returns)\n",
    "        qs_stats[\"Gain to Pain Ratio\"] = qs.stats.gain_to_pain_ratio(returns)\n",
    "        qs_stats[\"Ulcer Index\"] = qs.stats.ulcer_index(returns)\n",
    "        qs_stats[\"Consecutive Wins\"] = qs.stats.consecutive_wins(returns)\n",
    "        qs_stats[\"Consecutive Losses\"] = qs.stats.consecutive_losses(returns)\n",
    "        # ----------------- 新增指标 -----------------\n",
    "        qs_stats[\"Avg Return\"] = qs.stats.avg_return(returns)\n",
    "        qs_stats[\"CAGR\"] = qs.stats.cagr(returns)\n",
    "        qs_stats[\"Expected Shortfall\"] = qs.stats.expected_shortfall(returns)\n",
    "        qs_stats[\"Information Ratio\"] = qs.stats.information_ratio(returns)\n",
    "        qs_stats[\"Profit Ratio\"] = qs.stats.profit_ratio(returns)\n",
    "        qs_stats[\"R2\"] = qs.stats.r2(returns)\n",
    "        qs_stats[\"R Squared\"] = qs.stats.r_squared(returns)\n",
    "        qs_stats[\"Recovery Factor\"] = qs.stats.recovery_factor(returns)\n",
    "        qs_stats[\"Risk-Return Ratio\"] = qs.stats.risk_return_ratio(returns)\n",
    "        qs_stats[\"Win/Loss Ratio\"] = qs.stats.win_loss_ratio(returns)\n",
    "        qs_stats[\"Worst\"] = qs.stats.worst(returns)\n",
    "        # ------------------------------------------------\n",
    "    except Exception as e:\n",
    "        qs_stats[\"error\"] = str(e)\n",
    "\n",
    "    # 整合基础回测指标与量化收益指标\n",
    "    backtest_results = {\n",
    "        \"Initial Capital\": initial_capital,\n",
    "        \"Final Value\": final_value,\n",
    "        \"Profit\": profit,\n",
    "        \"ROI (%)\": roi,\n",
    "    }\n",
    "    backtest_results.update(qs_stats)\n",
    "\n",
    "    return backtest_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_master_results(config):\n",
    "    \"\"\"\n",
    "    加载全局优化结果文件，并提取已完成优化的组合（基于 'Symbol', 'Target Timeframe', 'Rank' 列）。\n",
    "    \"\"\"\n",
    "    # 构造 master 文件路径\n",
    "    start_clean = config['start_date'].replace(\"-\", \"\")\n",
    "    end_clean = config['end_date'].replace(\"-\", \"\")\n",
    "    master_file = os.path.join(\n",
    "        config['reports_path'], \n",
    "        config['results_filename_template'].format(\n",
    "            strategy_name=config['strategy']['name'],\n",
    "            start_date=start_clean,\n",
    "            end_date=end_clean\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if os.path.exists(master_file):\n",
    "        try:\n",
    "            master_df = pd.read_csv(master_file)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                master_df = pd.read_excel(master_file)\n",
    "            except Exception as e2:\n",
    "                print(f\"无法读取文件: {e}, {e2}\")\n",
    "                master_df = pd.DataFrame()\n",
    "    else:\n",
    "        master_df = pd.DataFrame()\n",
    "    \n",
    "    optimized_combinations = set()\n",
    "    if not master_df.empty:\n",
    "        if {'Target Timeframe', 'Symbol', 'Rank'}.issubset(master_df.columns):\n",
    "            for symbol in master_df['Symbol'].unique():\n",
    "                for tf in master_df['Target Timeframe'].unique():\n",
    "                    rows = master_df[(master_df['Symbol'] == symbol) & (master_df['Target Timeframe'] == tf)]\n",
    "                    ranks = rows['Rank'].tolist()\n",
    "                    # 当存在 5 个排名且排名为 1 到 5 时认为该组合已完成优化\n",
    "                    if len(ranks) == 5 and set(ranks) == set(range(1, 6)):\n",
    "                        optimized_combinations.add((symbol, tf))\n",
    "        else:\n",
    "            print(\"警告: 结果文件缺少必要的列，将重新开始优化\")\n",
    "    else:\n",
    "        print(\"优化结果文件为空\")\n",
    "        \n",
    "    return master_file, master_df, optimized_combinations\n",
    "\n",
    "def save_master_results(new_results, master_file):\n",
    "    \"\"\"\n",
    "    将新的优化结果追加到全局结果文件中，只进行追加操作，并避免重复数据。\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    if not new_results:\n",
    "        print(\"警告: 没有新的结果需要保存\")\n",
    "        return\n",
    "\n",
    "    new_df = pd.DataFrame(new_results)\n",
    "    \n",
    "    # 如果文件不存在，直接写入\n",
    "    if not os.path.exists(master_file):\n",
    "        new_df.to_csv(master_file, index=False)\n",
    "        print(f\"创建新文件并写入 {len(new_df)} 条记录\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 读取现有数据\n",
    "        existing_df = pd.read_csv(master_file)\n",
    "        \n",
    "        # 创建用于检查重复的键\n",
    "        def create_key(row):\n",
    "            return f\"{row['Symbol']}_{row['Target Timeframe']}_{row['Rank']}\"\n",
    "        \n",
    "        existing_keys = set(existing_df.apply(create_key, axis=1))\n",
    "        new_df['_temp_key'] = new_df.apply(create_key, axis=1)\n",
    "        \n",
    "        # 过滤掉重复的记录\n",
    "        truly_new_df = new_df[~new_df['_temp_key'].isin(existing_keys)]\n",
    "        truly_new_df = truly_new_df.drop('_temp_key', axis=1)\n",
    "        \n",
    "        if len(truly_new_df) > 0:\n",
    "            # 追加非重复的新数据\n",
    "            truly_new_df.to_csv(master_file, mode='a', header=False, index=False)\n",
    "            print(f\"成功追加 {len(truly_new_df)} 条新记录（过滤掉 {len(new_df) - len(truly_new_df)} 条重复记录）\")\n",
    "        else:\n",
    "            print(\"所有记录都已存在，无需追加\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理数据时出错: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        full_df = pd.read_csv(master_file)\n",
    "        print(f\"当前文件总共包含 {len(full_df)} 条记录\")\n",
    "    except Exception as e:\n",
    "        print(f\"验证追加结果时出错: {str(e)}\")\n",
    "\n",
    "def process_symbol_tf(symbol, tf, config):\n",
    "    \"\"\"\n",
    "    针对单个交易对和指定时间周期执行策略参数优化与回测，并赋予 1~5 的排名。\n",
    "    \"\"\"\n",
    "    print(f\"\\n开始针对 {symbol} 时间周期 {tf} 优化...\")\n",
    "    # 使用已有的 optimize_strategy 函数\n",
    "    top_results = optimize_strategy(symbol, tf)\n",
    "    if not top_results:\n",
    "        print(f\"警告: {symbol} 在 {tf} 时间周期下优化失败\")\n",
    "        return []\n",
    "    \n",
    "    processed_results = []\n",
    "    # 为每个参数组合运行回测并获取量化指标，同时赋予排名\n",
    "    for idx, res in enumerate(top_results, start=1):\n",
    "        res['Symbol'] = symbol\n",
    "        res['Target Timeframe'] = tf\n",
    "        res['Rank'] = idx\n",
    "        metrics = run_backtest_with_params(res, symbol, tf)\n",
    "        res.update(metrics)\n",
    "        processed_results.append(res)\n",
    "        \n",
    "    print(f\"完成 {symbol} 在 {tf} 时间周期下的优化，获得 {len(processed_results)} 个结果\")\n",
    "    return processed_results\n",
    "\n",
    "def process_symbol_batch(symbol, timeframes, config_path):\n",
    "    \"\"\"用于并行处理的包装函数，避免传递大型配置对象\"\"\"\n",
    "    try:\n",
    "        # 在子进程中重新加载配置，避免序列化问题\n",
    "        import json\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "            \n",
    "        # 导入必要的模块（每个进程需要单独导入）\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import optuna\n",
    "        import backtrader as bt\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        print(f\"开始处理交易对: {symbol}\")\n",
    "        # 验证数据完整性\n",
    "        if not verify_data_completeness(symbol, config['start_date'], config['end_date'], config['data_path']):\n",
    "            print(f\"警告: 跳过 {symbol} - 数据不完整\")\n",
    "            return []\n",
    "        \n",
    "        # 处理每个时间周期\n",
    "        for tf in timeframes:\n",
    "            print(f\"处理组合: {symbol}-{tf}\")\n",
    "            # 使用内存存储优化\n",
    "            top_results = optimize_strategy(symbol, tf)\n",
    "            if not top_results:\n",
    "                continue\n",
    "                \n",
    "            processed_results = []\n",
    "            # 为每个参数组合运行回测并获取量化指标\n",
    "            for idx, res in enumerate(top_results, start=1):\n",
    "                res['Symbol'] = symbol\n",
    "                res['Target Timeframe'] = tf\n",
    "                res['Rank'] = idx\n",
    "                metrics = run_backtest_with_params(res, symbol, tf)\n",
    "                res.update(metrics)\n",
    "                processed_results.append(res)\n",
    "            \n",
    "            results.extend(processed_results)\n",
    "            print(f\"完成组合 {symbol}-{tf} 的优化\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = f\"处理 {symbol} 时出错: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        # 返回错误信息而不是引发异常\n",
    "        return [{\"error\": error_msg, \"Symbol\": symbol}]\n",
    "\n",
    "def auto_backup_results(master_file, backup_folder='backups'):\n",
    "    \"\"\"\n",
    "    创建优化结果文件的自动备份\n",
    "    \"\"\"\n",
    "    if not os.path.exists(master_file):\n",
    "        print(f\"文件不存在，无需备份: {master_file}\")\n",
    "        return\n",
    "    \n",
    "    # 创建备份目录\n",
    "    os.makedirs(backup_folder, exist_ok=True)\n",
    "    \n",
    "    # 创建带时间戳的备份文件名\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.basename(master_file)\n",
    "    backup_file = os.path.join(backup_folder, f\"{filename}.{timestamp}.bak\")\n",
    "    \n",
    "    try:\n",
    "        import shutil\n",
    "        shutil.copy2(master_file, backup_file)\n",
    "        print(f\"已自动创建备份: {backup_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"自动备份失败: {str(e)}\")\n",
    "\n",
    "def batch_optimize(config):\n",
    "    \"\"\"\n",
    "    批量优化所有交易对和指定多个时间周期，使用多线程保持输出实时性\n",
    "    \"\"\"\n",
    "    os.makedirs(config['reports_path'], exist_ok=True)\n",
    "    \n",
    "    master_file, master_df, optimized_combinations = load_master_results(config)\n",
    "    \n",
    "    # 获取交易对列表\n",
    "    if config.get('selected_symbols'):\n",
    "        symbols = config['selected_symbols']\n",
    "    else:\n",
    "        symbols = get_all_symbols(config['data_path'], config['start_date'])\n",
    "        print(f\"总共找到 {len(symbols)} 个交易对\")\n",
    "    \n",
    "    # 开始批量优化前，先创建一个备份\n",
    "    auto_backup_results(master_file)\n",
    "    \n",
    "    # 准备未完成的组合列表\n",
    "    remaining_combinations = []\n",
    "    for symbol in symbols:\n",
    "        for tf in config['target_timeframes']:\n",
    "            if (symbol, tf) not in optimized_combinations:\n",
    "                remaining_combinations.append((symbol, tf))\n",
    "    \n",
    "    print(f\"剩余需要优化的组合数量: {len(remaining_combinations)} 个\")\n",
    "    \n",
    "    # 使用多线程并行处理\n",
    "    import threading\n",
    "    from queue import Queue\n",
    "    \n",
    "    # 创建任务队列\n",
    "    task_queue = Queue()\n",
    "    for combo in remaining_combinations:\n",
    "        task_queue.put(combo)\n",
    "    \n",
    "    # 创建结果队列\n",
    "    result_queue = Queue()\n",
    "    \n",
    "    # 添加线程安全锁，保护文件写入操作\n",
    "    file_lock = threading.Lock()\n",
    "    \n",
    "    # 线程处理函数\n",
    "    def worker():\n",
    "        while not task_queue.empty():\n",
    "            try:\n",
    "                symbol, tf = task_queue.get(block=False)\n",
    "                print(f\"开始处理组合: {symbol}-{tf}\")\n",
    "                results = process_symbol_tf(symbol, tf, config)\n",
    "                if results:\n",
    "                    result_queue.put(results)\n",
    "                task_queue.task_done()\n",
    "            except Exception as e:\n",
    "                print(f\"处理任务出错: {e}\")\n",
    "                try:\n",
    "                    task_queue.task_done()\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # 创建并启动多个线程\n",
    "    n_threads = min(30, len(remaining_combinations))  # 使用30个线程\n",
    "    print(f\"启动 {n_threads} 个并行线程处理任务\")\n",
    "    \n",
    "    threads = []\n",
    "    for _ in range(n_threads):\n",
    "        t = threading.Thread(target=worker)\n",
    "        t.daemon = True\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    # 定期保存结果并自动创建备份\n",
    "    import time\n",
    "    processed_count = 0\n",
    "    total_remaining = len(remaining_combinations)\n",
    "    last_backup_time = time.time()\n",
    "    backup_interval = 600  # 每10分钟备份一次\n",
    "    \n",
    "    while any(t.is_alive() for t in threads) or not result_queue.empty():\n",
    "        # 收集并保存累积的结果\n",
    "        batch_results = []\n",
    "        while not result_queue.empty():\n",
    "            try:\n",
    "                results = result_queue.get(block=False)\n",
    "                batch_results.extend(results)\n",
    "                result_queue.task_done()\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # 如果有结果，保存它们，使用互斥锁保护文件写入\n",
    "        if batch_results:\n",
    "            with file_lock:\n",
    "                save_master_results(batch_results, master_file)\n",
    "                # 在保存完成后重新加载最新的优化组合情况\n",
    "                _, _, updated_optimized_combinations = load_master_results(config)\n",
    "                optimized_combinations = updated_optimized_combinations\n",
    "            \n",
    "            processed_count = len(optimized_combinations)\n",
    "            print(f\"保存了 {len(batch_results)} 条结果记录，总进度: {processed_count}/{total_remaining} ({processed_count/total_remaining*100:.1f}%)\")\n",
    "        \n",
    "        # 定期创建备份\n",
    "        current_time = time.time()\n",
    "        if current_time - last_backup_time > backup_interval:\n",
    "            with file_lock:\n",
    "                auto_backup_results(master_file)\n",
    "            last_backup_time = current_time\n",
    "        \n",
    "        # 等待一小段时间\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # 最后一次收集结果\n",
    "    batch_results = []\n",
    "    while not result_queue.empty():\n",
    "        results = result_queue.get()\n",
    "        batch_results.extend(results)\n",
    "        result_queue.task_done()\n",
    "    \n",
    "    if batch_results:\n",
    "        with file_lock:\n",
    "            save_master_results(batch_results, master_file)\n",
    "        # 最终统计\n",
    "        _, _, final_optimized = load_master_results(config)\n",
    "        processed_count = len(final_optimized)\n",
    "    \n",
    "    # 在完成后创建最终备份\n",
    "    auto_backup_results(master_file, backup_folder='backups_final')\n",
    "    \n",
    "    print(f\"批量优化完成。共处理 {processed_count}/{total_remaining} 个组合。\")\n",
    "    \n",
    "    # 在完成后，最后再检查一次结果完整性\n",
    "    try:\n",
    "        final_df = pd.read_csv(master_file)\n",
    "        print(f\"最终结果文件包含 {len(final_df)} 条记录\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取最终结果文件时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增清理函数：清理不在最终结果 CSV 中的 optuna 数据库文件\n",
    "def clean_incomplete_optuna_db_files(config):\n",
    "    \"\"\"\n",
    "    清理不在最终优化结果 CSV 中的 optuna 数据库文件。\n",
    "    该函数会读取最终结果 CSV（如果存在），提取已完成优化的 (Symbol, Target Timeframe) 组合，\n",
    "    然后删除 reports 目录下不在该列表中的 optuna 数据库文件。\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from glob import glob\n",
    "\n",
    "    # 从最终结果 CSV 中加载已完成优化组合\n",
    "    master_file, master_df, optimized_combinations = load_master_results(config)\n",
    "    print(f\"已完成优化组合: {optimized_combinations}\")\n",
    "    \n",
    "    # 匹配与当前策略相关的 optuna 数据库文件\n",
    "    pattern = os.path.join(config['reports_path'], f\"optuna_{config['strategy']['name']}_*.db\")\n",
    "    db_files = glob(pattern)\n",
    "    for db_file in db_files:\n",
    "        filename = os.path.basename(db_file)\n",
    "        prefix = f\"optuna_{config['strategy']['name']}_\"\n",
    "        # 确保文件名格式正确\n",
    "        if filename.startswith(prefix) and filename.endswith(\".db\"):\n",
    "            # 去掉前缀和后缀，得到 \"symbol_timeframe\"\n",
    "            core = filename[len(prefix):-3]  # 去掉后面的 \".db\"\n",
    "            parts = core.rsplit(\"_\", 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            symbol, timeframe = parts\n",
    "            # 如果此组合不在最终结果中，则删除该数据库文件\n",
    "            if (symbol, timeframe) not in optimized_combinations:\n",
    "                try:\n",
    "                    os.remove(db_file)\n",
    "                    print(f\"已删除未完成优化的数据库文件：{db_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"删除文件 {db_file} 出错：{e}\")\n",
    "    print(\"数据库清理完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_from_backup(master_file, backup_folder='backups'):\n",
    "    \"\"\"\n",
    "    从备份恢复优化结果文件\n",
    "    \"\"\"\n",
    "    # 检查备份文件夹\n",
    "    if not os.path.exists(backup_folder):\n",
    "        print(f\"备份文件夹不存在: {backup_folder}\")\n",
    "        return False\n",
    "    \n",
    "    # 查找与主文件相关的所有备份\n",
    "    filename = os.path.basename(master_file)\n",
    "    backup_pattern = os.path.join(backup_folder, f\"{filename}.*.bak\")\n",
    "    import glob\n",
    "    backup_files = glob.glob(backup_pattern)\n",
    "    \n",
    "    if not backup_files:\n",
    "        print(f\"没有找到备份文件: {backup_pattern}\")\n",
    "        return False\n",
    "    \n",
    "    # 按修改时间排序，获取最新的备份\n",
    "    backup_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    latest_backup = backup_files[0]\n",
    "    \n",
    "    try:\n",
    "        # 读取备份文件\n",
    "        if latest_backup.endswith('.csv.bak'):\n",
    "            backup_df = pd.read_csv(latest_backup)\n",
    "        else:\n",
    "            backup_df = pd.read_excel(latest_backup)\n",
    "        \n",
    "        # 读取当前文件(如果存在)\n",
    "        current_df = pd.DataFrame()\n",
    "        if os.path.exists(master_file):\n",
    "            try:\n",
    "                current_df = pd.read_csv(master_file)\n",
    "            except:\n",
    "                print(f\"无法读取当前文件: {master_file}\")\n",
    "        \n",
    "        # 比较记录数\n",
    "        if current_df.empty or len(backup_df) > len(current_df):\n",
    "            print(f\"备份文件包含 {len(backup_df)} 条记录，当前文件包含 {len(current_df)} 条记录\")\n",
    "            \n",
    "            # 创建当前文件的备份（以防万一）\n",
    "            if os.path.exists(master_file) and not current_df.empty:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                current_backup = f\"{master_file}.before_restore.{timestamp}\"\n",
    "                import shutil\n",
    "                shutil.copy2(master_file, current_backup)\n",
    "                print(f\"在恢复前已创建当前文件的备份: {current_backup}\")\n",
    "            \n",
    "            # 恢复备份文件\n",
    "            import shutil\n",
    "            shutil.copy2(latest_backup, master_file)\n",
    "            print(f\"已从备份 {latest_backup} 恢复数据\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"当前文件记录数({len(current_df)})不少于备份文件({len(backup_df)})，无需恢复\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"恢复过程中出错: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 先清理不完整的 Optuna 数据库文件（不在最终结果 CSV 中的组合）\n",
    "    # clean_incomplete_optuna_db_files(CONFIG)\n",
    "    batch_optimize(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数据恢复工具 - 如果发现结果文件数据被意外清空，运行此单元格恢复数据\n",
    "\n",
    "# # 构造结果文件路径\n",
    "# start_clean = CONFIG['start_date'].replace(\"-\", \"\")\n",
    "# end_clean = CONFIG['end_date'].replace(\"-\", \"\")\n",
    "# results_file = os.path.join(\n",
    "#     CONFIG['reports_path'], \n",
    "#     CONFIG['results_filename_template'].format(\n",
    "#         strategy_name=CONFIG['strategy']['name'],\n",
    "#         start_date=start_clean,\n",
    "#         end_date=end_clean\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # 检查当前文件状态\n",
    "# current_records = 0\n",
    "# if os.path.exists(results_file):\n",
    "#     try:\n",
    "#         current_df = pd.read_csv(results_file)\n",
    "#         current_records = len(current_df)\n",
    "#     except:\n",
    "#         print(f\"无法读取当前结果文件: {results_file}\")\n",
    "# print(f\"当前结果文件状态: {'存在' if os.path.exists(results_file) else '不存在'}, 包含 {current_records} 条记录\")\n",
    "\n",
    "# # 从最新备份恢复\n",
    "# if current_records == 0 or input(\"是否要从备份恢复数据？(y/n): \").lower() == 'y':\n",
    "#     if restore_from_backup(results_file):\n",
    "#         print(\"数据恢复成功！\")\n",
    "#     else:\n",
    "#         print(\"数据恢复失败。尝试从另一个备份文件夹恢复...\")\n",
    "#         if restore_from_backup(results_file, backup_folder='backups_final'):\n",
    "#             print(\"数据从final备份成功恢复！\")\n",
    "#         else:\n",
    "#             print(\"所有恢复尝试均失败，请手动处理。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backtrader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
