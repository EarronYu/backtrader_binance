{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存币安合约历史数据\n",
    "\n",
    "本文档用于下载和保存币安合约的历史K线数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据获取参数\n",
    "params = {\n",
    "    # 数据获取配置\n",
    "    'begin_date': '2025-03-01',\n",
    "    'end_date': '2025-03-24',\n",
    "    'time_intervals': ['1m'],  # 可选: ['1m', '5m', '15m', '1h', '4h', '1d']\n",
    "    'use_all_usdt_pairs': False,  # 设置为True则获取所有USDT交易对\n",
    "    'specific_symbols': ['1000PEPE/USDT:USDT'],  # 当use_all_usdt_pairs为False时使用\n",
    "    'base_path': r'\\\\znas\\Main\\futures',  # 指定数据保存的根目录\n",
    "    \n",
    "    # 代理配置\n",
    "    'proxy': {\n",
    "        'host': '127.0.0.1',\n",
    "        'port': 5878\n",
    "    },\n",
    "    \n",
    "    # 交易所基础配置\n",
    "    'exchange_config': {\n",
    "        'timeout': 9474,\n",
    "        'enableRateLimit': False,\n",
    "        'options': {\n",
    "            'defaultType': 'future'  # 设置为合约模式\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 确保根目录存在\n",
    "os.makedirs(params['base_path'], exist_ok=True)\n",
    "\n",
    "# 生成日期列表\n",
    "start_date = datetime.strptime(params['begin_date'], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(params['end_date'], '%Y-%m-%d')\n",
    "date_list = pd.date_range(start=start_date, end=end_date, freq='D').strftime('%Y-%m-%d').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_existing_files(base_path):\n",
    "    \"\"\"预扫描已存在的文件\"\"\"\n",
    "    existing_files = set()\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                existing_files.add(file)\n",
    "    print(f\"已扫描到 {len(existing_files)} 个现有文件\")\n",
    "    return existing_files\n",
    "\n",
    "def analyze_download_status(target_symbols, existing_files, date_list, time_intervals):\n",
    "    \"\"\"分析每个交易对的下载情况\"\"\"\n",
    "    stats = {}\n",
    "    for symbol in target_symbols:\n",
    "        stats[symbol] = {\n",
    "            'total_expected': len(date_list) * len(time_intervals),\n",
    "            'downloaded': 0,\n",
    "            'missing_dates': []\n",
    "        }\n",
    "        \n",
    "        for start_time in date_list:\n",
    "            for time_interval in time_intervals:\n",
    "                date_str = str(pd.to_datetime(start_time).date())\n",
    "                file_name = f\"{date_str}_{symbol.replace('/', '_').replace(':', '_')}_{time_interval}.csv\"\n",
    "                if file_name in existing_files:\n",
    "                    stats[symbol]['downloaded'] += 1\n",
    "                else:\n",
    "                    stats[symbol]['missing_dates'].append(f\"{date_str}_{time_interval}\")\n",
    "\n",
    "    print(\"\\n下载统计信息:\")\n",
    "    incomplete_symbols = []\n",
    "    for symbol, data in stats.items():\n",
    "        completion_rate = (data['downloaded'] / data['total_expected']) * 100\n",
    "        print(f\"{symbol}: 完成率 {completion_rate:.2f}% ({data['downloaded']}/{data['total_expected']})\")\n",
    "        if data['downloaded'] < data['total_expected']:\n",
    "            incomplete_symbols.append(symbol)\n",
    "            if len(data['missing_dates']) <= 10:\n",
    "                print(f\"  缺失数据: {data['missing_dates'][:10]}\")\n",
    "            else:\n",
    "                print(f\"  缺失数据过多，共{len(data['missing_dates'])}个日期\")\n",
    "\n",
    "    print(f\"\\n未完全下载的交易对数量: {len(incomplete_symbols)}/{len(target_symbols)}\")\n",
    "    return incomplete_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_exchange():\n",
    "    \"\"\"初始化交易所接口\"\"\"\n",
    "    config = {\n",
    "        **params['exchange_config'],\n",
    "        'proxies': {\n",
    "            'http': f\"http://{params['proxy']['host']}:{params['proxy']['port']}\",\n",
    "            'https': f\"http://{params['proxy']['host']}:{params['proxy']['port']}\"\n",
    "        }\n",
    "    }\n",
    "    return ccxt.binance(config)\n",
    "\n",
    "exchange = init_exchange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_symbols():\n",
    "    \"\"\"获取可用的交易对列表\"\"\"\n",
    "    if params['use_all_usdt_pairs']:\n",
    "        markets = exchange.load_markets()\n",
    "        return [symbol for symbol in markets.keys() if ':USDT' in symbol]\n",
    "    else:\n",
    "        return params['specific_symbols']\n",
    "\n",
    "def fetch_and_save_data(symbol, timeframe, start_time):\n",
    "    \"\"\"获取并保存单个交易对的数据\"\"\"\n",
    "    try:\n",
    "        # 构建文件名和路径\n",
    "        date_str = str(pd.to_datetime(start_time).date())\n",
    "        date_path = os.path.join(params['base_path'], date_str)\n",
    "        os.makedirs(date_path, exist_ok=True)\n",
    "        \n",
    "        file_name = f\"{date_str}_{symbol.replace('/', '').replace(':', '_')}_{timeframe}.csv\"\n",
    "        save_path = os.path.join(date_path, file_name)\n",
    "        \n",
    "        # 如果文件已存在，跳过\n",
    "        if os.path.exists(save_path):\n",
    "            # print(f\"文件 {file_name} 已存在，跳过下载\")\n",
    "            return True, None\n",
    "        \n",
    "        # 获取数据\n",
    "        print(f'正在获取 {exchange.id} {symbol} {timeframe} {start_time} 的数据')\n",
    "        since = int(pd.Timestamp(f'{start_time} 00:00:00').timestamp() * 1000)\n",
    "        end = int(pd.Timestamp(f'{start_time} 23:59:59').timestamp() * 1000)\n",
    "        \n",
    "        all_data = []\n",
    "        current_since = since\n",
    "        \n",
    "        while current_since < end:\n",
    "            data = exchange.fetch_ohlcv(\n",
    "                symbol=symbol,\n",
    "                timeframe=timeframe,\n",
    "                since=current_since,\n",
    "                limit=1000\n",
    "            )\n",
    "            \n",
    "            if not data:\n",
    "                break\n",
    "                \n",
    "            all_data.extend(data)\n",
    "            current_since = data[-1][0] + 1\n",
    "            time.sleep(0.9)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(f\"{symbol} 在 {start_time} 无数据\")\n",
    "            return False, None\n",
    "            \n",
    "        # 转换为DataFrame并保存\n",
    "        df = pd.DataFrame(\n",
    "            all_data,\n",
    "            columns=['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "        )\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], unit='ms')\n",
    "        \n",
    "        # 筛选当天数据\n",
    "        target_date = pd.to_datetime(start_time).date()\n",
    "        df = df[df['datetime'].dt.date == target_date]\n",
    "        \n",
    "        # 数据清理\n",
    "        df = df.drop_duplicates(subset=['datetime'], keep='last')\\\n",
    "               .sort_values('datetime')\\\n",
    "               .reset_index(drop=True)\n",
    "        \n",
    "        if not df.empty:\n",
    "            df.to_csv(save_path, index=False)\n",
    "            # print(f'成功下载并保存 {symbol} 在 {start_time} 的数据，数据形状: {df.shape}')\n",
    "            return True, df\n",
    "        else:\n",
    "            print(f\"{symbol} 在 {start_time} 筛选后无数据\")\n",
    "            return False, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'获取数据失败: {symbol}_{timeframe}_{start_time}, 错误: {e}')\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已扫描到 203197 个现有文件\n",
      "将处理以下交易对: ['1000PEPE/USDT:USDT']\n",
      "正在获取 binance 1000PEPE/USDT:USDT 1m 2025-03-24 的数据\n",
      "正在获取 binance 1000PEPE/USDT:USDT 1m 2025-03-23 的数据\n",
      "正在获取 binance 1000PEPE/USDT:USDT 1m 2025-03-22 的数据\n",
      "正在获取 binance 1000PEPE/USDT:USDT 1m 2025-03-21 的数据\n",
      "正在获取 binance 1000PEPE/USDT:USDT 1m 2025-03-20 的数据\n",
      "正在获取 binance 1000PEPE/USDT:USDT 1m 2025-03-19 的数据\n",
      "\n",
      "开始分析下载情况...\n",
      "已扫描到 203203 个现有文件\n",
      "\n",
      "下载统计信息:\n",
      "1000PEPE/USDT:USDT: 完成率 0.00% (0/24)\n",
      "  缺失数据过多，共24个日期\n",
      "\n",
      "未完全下载的交易对数量: 1/1\n",
      "\n",
      "是否要重新下载未完成的交易对？(y/n)\n",
      "\n",
      "开始重新下载未完成的交易对...\n"
     ]
    }
   ],
   "source": [
    "# 获取现有文件列表\n",
    "existing_files = scan_existing_files(params['base_path'])\n",
    "\n",
    "# 获取要处理的交易对\n",
    "target_symbols = get_available_symbols()\n",
    "print(f\"将处理以下交易对: {target_symbols}\")\n",
    "\n",
    "# 主循环\n",
    "error_list = []\n",
    "empty_data_count = {}  # 用于记录每个交易对的连续空数据天数\n",
    "\n",
    "# 反转日期列表，从最新日期开始获取\n",
    "date_list.reverse()\n",
    "\n",
    "for symbol in target_symbols:\n",
    "    empty_data_count[symbol] = 0\n",
    "    \n",
    "    for start_time in date_list:\n",
    "        if empty_data_count[symbol] >= 3:\n",
    "            print(f'{symbol} 连续 {empty_data_count[symbol]} 天无数据，跳转到下一个交易对')\n",
    "            break\n",
    "            \n",
    "        for time_interval in params['time_intervals']:\n",
    "            success, df = fetch_and_save_data(symbol, time_interval, start_time)\n",
    "            \n",
    "            if not success:\n",
    "                error_msg = f'{exchange.id}_{symbol}_{time_interval}_{start_time}'\n",
    "                error_list.append(error_msg)\n",
    "                empty_data_count[symbol] += 1\n",
    "            else:\n",
    "                empty_data_count[symbol] = 0\n",
    "                \n",
    "        if empty_data_count[symbol] >= 3:\n",
    "            break\n",
    "\n",
    "# 分析下载情况\n",
    "print(\"\\n开始分析下载情况...\")\n",
    "existing_files = scan_existing_files(params['base_path'])  # 重新扫描\n",
    "incomplete_symbols = analyze_download_status(\n",
    "    target_symbols,\n",
    "    existing_files,\n",
    "    date_list,\n",
    "    params['time_intervals']\n",
    ")\n",
    "\n",
    "# 询问是否重新下载未完成的交易对\n",
    "if incomplete_symbols:\n",
    "    print(\"\\n是否要重新下载未完成的交易对？(y/n)\")\n",
    "    if input().lower() == 'y':\n",
    "        target_symbols = incomplete_symbols\n",
    "        # 重新运行主循环\n",
    "        print(\"\\n开始重新下载未完成的交易对...\")\n",
    "        for symbol in target_symbols:\n",
    "            empty_data_count[symbol] = 0\n",
    "            \n",
    "            for start_time in date_list:\n",
    "                if empty_data_count[symbol] >= 3:\n",
    "                    print(f'{symbol} 连续 {empty_data_count[symbol]} 天无数据，跳转到下一个交易对')\n",
    "                    break\n",
    "                    \n",
    "                for time_interval in params['time_intervals']:\n",
    "                    success, df = fetch_and_save_data(symbol, time_interval, start_time)\n",
    "                    \n",
    "                    if not success:\n",
    "                        error_msg = f'{exchange.id}_{symbol}_{time_interval}_{start_time}'\n",
    "                        error_list.append(error_msg)\n",
    "                        empty_data_count[symbol] += 1\n",
    "                    else:\n",
    "                        empty_data_count[symbol] = 0\n",
    "                        \n",
    "                if empty_data_count[symbol] >= 3:\n",
    "                    break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据可用性矩阵已保存到: \\\\znas\\Main\\futures\\data_availability_matrix.csv\n",
      "\n",
      "统计信息:\n",
      "总交易对数量: 404\n",
      "总日期数量: 814\n",
      "有数据的点数量: 202691\n",
      "数据覆盖率: 61.64%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "def create_data_availability_matrix(root_path):\n",
    "    # 获取所有日期文件夹\n",
    "    date_folders = glob.glob(os.path.join(root_path, '*'))\n",
    "    \n",
    "    # 收集所有交易对和日期\n",
    "    all_pairs = set()\n",
    "    all_dates = set()\n",
    "    data_dict = {}\n",
    "    \n",
    "    for folder in date_folders:\n",
    "        date = os.path.basename(folder)\n",
    "        try:\n",
    "            datetime.strptime(date, '%Y-%m-%d')  # 验证是否为日期格式\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        all_dates.add(date)\n",
    "        csv_files = glob.glob(os.path.join(folder, f'{date}_*_USDT_1m.csv'))\n",
    "        \n",
    "        for file in csv_files:\n",
    "            # 从文件名中提取交易对名称\n",
    "            filename = os.path.basename(file)\n",
    "            pair = filename.split('_')[1]\n",
    "            all_pairs.add(pair)\n",
    "            \n",
    "            # 文件存在即标记为1\n",
    "            if date not in data_dict:\n",
    "                data_dict[date] = {}\n",
    "            data_dict[date][pair] = 1\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    all_dates = sorted(list(all_dates))\n",
    "    all_pairs = sorted(list(all_pairs))\n",
    "    \n",
    "    df = pd.DataFrame(index=all_pairs, columns=all_dates)\n",
    "    \n",
    "    # 填充数据\n",
    "    for date in all_dates:\n",
    "        for pair in all_pairs:\n",
    "            df.loc[pair, date] = data_dict.get(date, {}).get(pair, 0)\n",
    "    \n",
    "    # 保存结果\n",
    "    output_file = os.path.join(root_path, 'data_availability_matrix.csv')\n",
    "    df.to_csv(output_file)\n",
    "    \n",
    "    print(f\"数据可用性矩阵已保存到: {output_file}\")\n",
    "    \n",
    "    # 打印统计信息\n",
    "    total_pairs = len(all_pairs)\n",
    "    total_dates = len(all_dates)\n",
    "    complete_data_points = (df == 1).sum().sum()\n",
    "    \n",
    "    print(f\"\\n统计信息:\")\n",
    "    print(f\"总交易对数量: {total_pairs}\")\n",
    "    print(f\"总日期数量: {total_dates}\")\n",
    "    print(f\"有数据的点数量: {complete_data_points}\")\n",
    "    print(f\"数据覆盖率: {(complete_data_points/(total_pairs*total_dates)*100):.2f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 使用示例\n",
    "root_path = r\"\\\\znas\\Main\\futures\"\n",
    "matrix = create_data_availability_matrix(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据可用性矩阵已保存到: \\\\znas\\Main\\futures\\data_availability_matrix.csv\n",
      "\n",
      "基本统计信息:\n",
      "总交易对数量: 404\n",
      "总日期数量: 814\n",
      "有数据的点数量: 202691\n",
      "数据覆盖率: 61.64%\n",
      "\n",
      "数据缺失分析:\n",
      "发现 0 个交易对存在不连续的数据缺失\n",
      "\n",
      "需要重新下载的数据清单已保存到: \\\\znas\\Main\\futures\\redownload_list.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def analyze_data_gaps(df, max_continuous_gap=10, ignore_tail_gaps=True):\n",
    "    \"\"\"\n",
    "    分析数据缺口\n",
    "    df: 数据可用性矩阵\n",
    "    max_continuous_gap: 最大连续空值天数，超过这个天数的空值认为是历史数据真实不存在\n",
    "    ignore_tail_gaps: 是否忽略尾部的连续空值（历史数据通常从某个时间点开始）\n",
    "    \"\"\"\n",
    "    problematic_pairs = {}\n",
    "    \n",
    "    for pair in df.index:\n",
    "        row_data = df.loc[pair].values\n",
    "        gaps = []\n",
    "        gap_start = None\n",
    "        continuous_zeros = 0\n",
    "        \n",
    "        for i, value in enumerate(row_data):\n",
    "            if value == 0:\n",
    "                continuous_zeros += 1\n",
    "                if gap_start is None:\n",
    "                    gap_start = df.columns[i]\n",
    "            else:\n",
    "                if gap_start is not None:\n",
    "                    # 如果连续空值天数小于阈值，记录这个缺口\n",
    "                    if continuous_zeros < max_continuous_gap:\n",
    "                        gaps.append({\n",
    "                            'start_date': gap_start,\n",
    "                            'end_date': df.columns[i-1],\n",
    "                            'days': continuous_zeros\n",
    "                        })\n",
    "                gap_start = None\n",
    "                continuous_zeros = 0\n",
    "        \n",
    "        # 处理最后的空值序列\n",
    "        if gap_start is not None and not ignore_tail_gaps:\n",
    "            if continuous_zeros < max_continuous_gap:\n",
    "                gaps.append({\n",
    "                    'start_date': gap_start,\n",
    "                    'end_date': df.columns[-1],\n",
    "                    'days': continuous_zeros\n",
    "                })\n",
    "        \n",
    "        # 如果存在需要处理的缺口，添加到问题交易对列表\n",
    "        if gaps:\n",
    "            problematic_pairs[pair] = gaps\n",
    "    \n",
    "    return problematic_pairs\n",
    "\n",
    "def create_data_availability_matrix(root_path):\n",
    "    # 获取所有日期文件夹\n",
    "    date_folders = glob.glob(os.path.join(root_path, '*'))\n",
    "    \n",
    "    # 收集所有交易对和日期\n",
    "    all_pairs = set()\n",
    "    all_dates = set()\n",
    "    data_dict = {}\n",
    "    \n",
    "    for folder in date_folders:\n",
    "        date = os.path.basename(folder)\n",
    "        try:\n",
    "            datetime.strptime(date, '%Y-%m-%d')  # 验证是否为日期格式\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        all_dates.add(date)\n",
    "        csv_files = glob.glob(os.path.join(folder, f'{date}_*_USDT_1m.csv'))\n",
    "        \n",
    "        for file in csv_files:\n",
    "            # 从文件名中提取交易对名称\n",
    "            filename = os.path.basename(file)\n",
    "            pair = filename.split('_')[1]\n",
    "            all_pairs.add(pair)\n",
    "            \n",
    "            # 文件存在即标记为1\n",
    "            if date not in data_dict:\n",
    "                data_dict[date] = {}\n",
    "            data_dict[date][pair] = 1\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    all_dates = sorted(list(all_dates))\n",
    "    all_pairs = sorted(list(all_pairs))\n",
    "    \n",
    "    df = pd.DataFrame(index=all_pairs, columns=all_dates)\n",
    "    \n",
    "    # 填充数据\n",
    "    for date in all_dates:\n",
    "        for pair in all_pairs:\n",
    "            df.loc[pair, date] = data_dict.get(date, {}).get(pair, 0)\n",
    "    \n",
    "    # 保存结果\n",
    "    output_file = os.path.join(root_path, 'data_availability_matrix.csv')\n",
    "    df.to_csv(output_file)\n",
    "    \n",
    "    print(f\"数据可用性矩阵已保存到: {output_file}\")\n",
    "    \n",
    "    # 分析数据缺口\n",
    "    problematic_pairs = analyze_data_gaps(df)\n",
    "    \n",
    "    # 打印统计信息\n",
    "    total_pairs = len(all_pairs)\n",
    "    total_dates = len(all_dates)\n",
    "    complete_data_points = (df == 1).sum().sum()\n",
    "    \n",
    "    print(f\"\\n基本统计信息:\")\n",
    "    print(f\"总交易对数量: {total_pairs}\")\n",
    "    print(f\"总日期数量: {total_dates}\")\n",
    "    print(f\"有数据的点数量: {complete_data_points}\")\n",
    "    print(f\"数据覆盖率: {(complete_data_points/(total_pairs*total_dates)*100):.2f}%\")\n",
    "    \n",
    "    print(f\"\\n数据缺失分析:\")\n",
    "    print(f\"发现 {len(problematic_pairs)} 个交易对存在不连续的数据缺失\")\n",
    "    \n",
    "    # 保存需要重新获取数据的清单\n",
    "    redownload_list = []\n",
    "    for pair, gaps in problematic_pairs.items():\n",
    "        print(f\"\\n交易对 {pair} 的数据缺口:\")\n",
    "        for gap in gaps:\n",
    "            print(f\"  从 {gap['start_date']} 到 {gap['end_date']} (共 {gap['days']} 天)\")\n",
    "            # 将每个缺失的日期都添加到重新下载列表\n",
    "            current_date = datetime.strptime(gap['start_date'], '%Y-%m-%d')\n",
    "            end_date = datetime.strptime(gap['end_date'], '%Y-%m-%d')\n",
    "            while current_date <= end_date:\n",
    "                redownload_list.append({\n",
    "                    'symbol': pair,\n",
    "                    'date': current_date.strftime('%Y-%m-%d')\n",
    "                })\n",
    "                current_date = current_date + pd.Timedelta(days=1)\n",
    "    \n",
    "    # 保存需要重新下载的数据清单\n",
    "    redownload_df = pd.DataFrame(redownload_list)\n",
    "    redownload_file = os.path.join(root_path, 'redownload_list.csv')\n",
    "    redownload_df.to_csv(redownload_file, index=False)\n",
    "    print(f\"\\n需要重新下载的数据清单已保存到: {redownload_file}\")\n",
    "    \n",
    "    return df, problematic_pairs\n",
    "\n",
    "# 使用示例\n",
    "root_path = r\"\\\\znas\\Main\\futures\"\n",
    "matrix, problems = create_data_availability_matrix(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m所有数据已完整补充！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# 运行补充下载\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[43mredownload_missing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mredownload_missing_data\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m未找到重新下载清单文件！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m redownload_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredownload_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 按交易对分组\u001b[39;00m\n\u001b[0;32m     18\u001b[0m grouped_downloads \u001b[38;5;241m=\u001b[39m redownload_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\x7498\\anaconda3\\envs\\backtrader\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\x7498\\anaconda3\\envs\\backtrader\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\x7498\\anaconda3\\envs\\backtrader\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\x7498\\anaconda3\\envs\\backtrader\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\x7498\\anaconda3\\envs\\backtrader\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\x7498\\anaconda3\\envs\\backtrader\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:555\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "def redownload_missing_data(params):\n",
    "    \"\"\"\n",
    "    根据redownload_list.csv补充下载缺失数据\n",
    "    复用原有的fetch_and_save_data函数\n",
    "    \"\"\"\n",
    "    # 初始化交易所\n",
    "    exchange = init_exchange()\n",
    "    \n",
    "    # 读取需要重新下载的数据清单\n",
    "    redownload_file = os.path.join(params['base_path'], 'redownload_list.csv')\n",
    "    if not os.path.exists(redownload_file):\n",
    "        print(\"未找到重新下载清单文件！\")\n",
    "        return\n",
    "    \n",
    "    redownload_df = pd.read_csv(redownload_file)\n",
    "    \n",
    "    # 按交易对分组\n",
    "    grouped_downloads = redownload_df.groupby('symbol')\n",
    "    \n",
    "    print(f\"开始补充下载缺失数据...\")\n",
    "    print(f\"共有 {len(grouped_downloads)} 个交易对需要补充数据\")\n",
    "    \n",
    "    # 用于记录错误\n",
    "    error_list = []\n",
    "    empty_data_count = {}\n",
    "    \n",
    "    # 获取可用的交易对列表\n",
    "    available_markets = exchange.load_markets()\n",
    "    \n",
    "    for symbol, group in grouped_downloads:\n",
    "        print(f\"\\n处理交易对: {symbol}\")\n",
    "        empty_data_count[symbol] = 0\n",
    "        dates = sorted(group['date'].unique())\n",
    "        \n",
    "        # 构建交易所格式的symbol\n",
    "        base_symbol = symbol.replace('USDT', '')  # 移除USDT后缀\n",
    "        exchange_symbol = f\"{base_symbol}/USDT:USDT\"\n",
    "        \n",
    "        # 检查交易对是否可用\n",
    "        if exchange_symbol not in available_markets:\n",
    "            print(f\"交易对 {exchange_symbol} 在交易所中不可用，跳过\")\n",
    "            continue\n",
    "        \n",
    "        for start_time in dates:\n",
    "            if empty_data_count[symbol] >= 3:\n",
    "                print(f'{symbol} 连续 {empty_data_count[symbol]} 天无数据，跳转到下一个交易对')\n",
    "                break\n",
    "                \n",
    "            print(f\"  下载 {start_time} 的数据...\")\n",
    "            \n",
    "            # 删除可能存在的不完整文件\n",
    "            date_str = str(pd.to_datetime(start_time).date())\n",
    "            date_path = os.path.join(params['base_path'], date_str)\n",
    "            os.makedirs(date_path, exist_ok=True)\n",
    "            \n",
    "            for time_interval in params['time_intervals']:\n",
    "                success, df = fetch_and_save_data(exchange_symbol, time_interval, start_time)\n",
    "                \n",
    "                if not success:\n",
    "                    error_msg = f'{exchange.id}_{symbol}_{time_interval}_{start_time}'\n",
    "                    error_list.append(error_msg)\n",
    "                    empty_data_count[symbol] += 1\n",
    "                    print(f\"    {time_interval} 数据下载失败\")\n",
    "                else:\n",
    "                    empty_data_count[symbol] = 0\n",
    "                    print(f\"    {time_interval} 数据下载成功\")\n",
    "            \n",
    "            if empty_data_count[symbol] >= 3:\n",
    "                break\n",
    "    \n",
    "    # 保存下载失败的记录\n",
    "    if error_list:\n",
    "        error_file = os.path.join(params['base_path'], 'redownload_errors.txt')\n",
    "        with open(error_file, 'w') as f:\n",
    "            f.write('\\n'.join(error_list))\n",
    "        print(f\"\\n仍有部分数据下载失败，详细信息已保存到: {error_file}\")\n",
    "    else:\n",
    "        print(\"\\n所有缺失数据补充完成！\")\n",
    "    \n",
    "    # 重新生成可用性矩阵\n",
    "    print(\"\\n重新生成数据可用性矩阵...\")\n",
    "    matrix, problems = create_data_availability_matrix(params['base_path'])\n",
    "    \n",
    "    # 检查是否还有问题数据\n",
    "    if problems:\n",
    "        print(f\"\\n警告：仍有 {len(problems)} 个交易对存在数据缺失\")\n",
    "        print(\"建议检查 data_availability_matrix.csv 查看具体情况\")\n",
    "    else:\n",
    "        print(\"\\n所有数据已完整补充！\")\n",
    "\n",
    "# 运行补充下载\n",
    "redownload_missing_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backtrader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
