{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置参数\n",
    "DATABASE_PATH = 'binance_futures_data.db'\n",
    "DATA_ROOT = r'\\\\znas\\Main\\futures'\n",
    "MAX_WORKERS = 32  # 减少线程数以减轻数据库争用\n",
    "BATCH_SIZE = 5000  # 每500个文件显示一次详细进度\n",
    "\n",
    "# 重试参数\n",
    "MAX_RETRIES = 5   # 最大重试次数\n",
    "RETRY_DELAY = 1   # 基础重试延迟秒数\n",
    "LOCK_WAIT_TIMEOUT = 20  # 锁等待超时秒数\n",
    "\n",
    "# 批量操作参数\n",
    "PRAGMA_SETTINGS = {\n",
    "    'journal_mode': 'WAL',      # 写入提前日志模式\n",
    "    'synchronous': 'NORMAL',    # 减少同步操作\n",
    "    'cache_size': 500000,        # 缓存大小(约50MB)\n",
    "    'temp_store': 'MEMORY',     # 临时存储使用内存\n",
    "    'busy_timeout': 30000,      # 繁忙等待超时(毫秒)\n",
    "    'locking_mode': 'NORMAL',   # 标准锁定模式\n",
    "}\n",
    "\n",
    "# 全局变量跟踪已处理文件\n",
    "processed_files = set()\n",
    "is_interrupted = False\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    \"\"\"处理Ctrl+C中断\"\"\"\n",
    "    global is_interrupted\n",
    "    print(\"\\n程序接收到中断信号，正在保存进度并安全退出...\")\n",
    "    is_interrupted = True\n",
    "\n",
    "# 注册信号处理器\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "def optimize_connection(conn):\n",
    "    \"\"\"优化SQLite连接参数以提高性能\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    for setting, value in PRAGMA_SETTINGS.items():\n",
    "        cursor.execute(f\"PRAGMA {setting} = {value}\")\n",
    "    conn.commit()\n",
    "\n",
    "def get_connection_with_retry(max_retries=MAX_RETRIES):\n",
    "    \"\"\"创建数据库连接，支持重试机制\"\"\"\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            conn = sqlite3.connect(DATABASE_PATH, check_same_thread=False, timeout=LOCK_WAIT_TIMEOUT)\n",
    "            optimize_connection(conn)\n",
    "            return conn\n",
    "        except sqlite3.Error as e:\n",
    "            if retry < max_retries - 1:\n",
    "                delay = RETRY_DELAY * (2 ** retry) + random.uniform(0, 1)  # 指数退避策略\n",
    "                print(f\"连接数据库失败: {str(e)}，{delay:.2f}秒后重试... ({retry+1}/{max_retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise sqlite3.Error(\"无法连接数据库，已达到最大重试次数\")\n",
    "\n",
    "def execute_with_retry(cursor, sql, params=None, max_retries=MAX_RETRIES):\n",
    "    \"\"\"执行SQL语句，支持重试机制\"\"\"\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            if params:\n",
    "                return cursor.execute(sql, params)\n",
    "            else:\n",
    "                return cursor.execute(sql)\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if \"database is locked\" in str(e) and retry < max_retries - 1:\n",
    "                delay = RETRY_DELAY * (2 ** retry) + random.uniform(0, 1)\n",
    "                print(f\"数据库锁定: {str(e)}，{delay:.2f}秒后重试... ({retry+1}/{max_retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def executemany_with_retry(cursor, sql, params_list, max_retries=MAX_RETRIES):\n",
    "    \"\"\"执行批量SQL语句，支持重试机制\"\"\"\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            return cursor.executemany(sql, params_list)\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if \"database is locked\" in str(e) and retry < max_retries - 1:\n",
    "                delay = RETRY_DELAY * (2 ** retry) + random.uniform(0, 1)\n",
    "                print(f\"数据库锁定: {str(e)}，{delay:.2f}秒后重试... ({retry+1}/{max_retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"创建SQLite数据库和表结构\"\"\"\n",
    "    conn = get_connection_with_retry()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # 创建主表存储价格数据\n",
    "    execute_with_retry(cursor, '''\n",
    "    CREATE TABLE IF NOT EXISTS price_data (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        symbol_id INTEGER,\n",
    "        datetime TEXT,\n",
    "        open REAL,\n",
    "        high REAL,\n",
    "        low REAL,\n",
    "        close REAL,\n",
    "        volume REAL,\n",
    "        file_path TEXT,  -- 添加文件路径以便去重\n",
    "        FOREIGN KEY (symbol_id) REFERENCES symbols(id)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # 创建符号表存储交易对信息\n",
    "    execute_with_retry(cursor, '''\n",
    "    CREATE TABLE IF NOT EXISTS symbols (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        symbol TEXT,\n",
    "        quote_asset TEXT,\n",
    "        timeframe TEXT,\n",
    "        UNIQUE(symbol, quote_asset, timeframe)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # 创建已处理文件表\n",
    "    execute_with_retry(cursor, '''\n",
    "    CREATE TABLE IF NOT EXISTS processed_files (\n",
    "        file_path TEXT PRIMARY KEY,\n",
    "        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # 创建索引以加快查询速度\n",
    "    execute_with_retry(cursor, 'CREATE INDEX IF NOT EXISTS idx_price_data_symbol_datetime ON price_data (symbol_id, datetime)')\n",
    "    execute_with_retry(cursor, 'CREATE INDEX IF NOT EXISTS idx_symbols_symbol ON symbols (symbol)')\n",
    "    execute_with_retry(cursor, 'CREATE INDEX IF NOT EXISTS idx_price_data_file_path ON price_data (file_path)')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def load_processed_files():\n",
    "    \"\"\"从数据库加载已处理的文件列表\"\"\"\n",
    "    global processed_files\n",
    "    \n",
    "    # 先检查数据库是否存在\n",
    "    if not os.path.exists(DATABASE_PATH):\n",
    "        return\n",
    "    \n",
    "    conn = get_connection_with_retry()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # 检查表是否存在\n",
    "    execute_with_retry(cursor, \"SELECT name FROM sqlite_master WHERE type='table' AND name='processed_files'\")\n",
    "    if cursor.fetchone() is None:\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    # 加载已处理文件列表\n",
    "    execute_with_retry(cursor, \"SELECT file_path FROM processed_files\")\n",
    "    for row in cursor.fetchall():\n",
    "        processed_files.add(row[0])\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"从数据库中加载了 {len(processed_files)} 个已处理文件记录\")\n",
    "\n",
    "def mark_file_as_processed(conn, file_path):\n",
    "    \"\"\"将文件标记为已处理\"\"\"\n",
    "    global processed_files\n",
    "    \n",
    "    # 添加到已处理集合\n",
    "    processed_files.add(file_path)\n",
    "    \n",
    "    # 添加到数据库\n",
    "    cursor = conn.cursor()\n",
    "    execute_with_retry(cursor, \"INSERT OR REPLACE INTO processed_files (file_path) VALUES (?)\", (file_path,))\n",
    "    conn.commit()\n",
    "\n",
    "def get_or_create_symbol_id(conn, symbol, quote_asset, timeframe):\n",
    "    \"\"\"获取或创建交易对ID\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    execute_with_retry(cursor,\n",
    "        'SELECT id FROM symbols WHERE symbol = ? AND quote_asset = ? AND timeframe = ?',\n",
    "        (symbol, quote_asset, timeframe)\n",
    "    )\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        execute_with_retry(cursor,\n",
    "            'INSERT INTO symbols (symbol, quote_asset, timeframe) VALUES (?, ?, ?)',\n",
    "            (symbol, quote_asset, timeframe)\n",
    "        )\n",
    "        conn.commit()\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def extract_file_info(filename):\n",
    "    \"\"\"从文件名中提取交易对、引用资产和时间帧信息\"\"\"\n",
    "    # 提取基本文件名，不包括路径\n",
    "    base_filename = os.path.basename(filename)\n",
    "    \n",
    "    # 使用正则表达式提取信息\n",
    "    match = re.match(r'(\\d{4}-\\d{2}-\\d{2})_(.+)_([A-Z]+)_([a-zA-Z0-9]+)\\.csv', base_filename)\n",
    "    if match:\n",
    "        date, symbol, quote_asset, timeframe = match.groups()\n",
    "        return {\n",
    "            'date': date,\n",
    "            'symbol': symbol,\n",
    "            'quote_asset': quote_asset,\n",
    "            'timeframe': timeframe\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def process_csv_file(file_path, file_index, total_files):\n",
    "    \"\"\"处理单个CSV文件并将数据存储到数据库\"\"\"\n",
    "    global is_interrupted\n",
    "    \n",
    "    # 检查是否被中断\n",
    "    if is_interrupted:\n",
    "        return {\"processed\": 0, \"file_path\": file_path, \"status\": \"中断\"}\n",
    "    \n",
    "    # 检查文件是否已处理\n",
    "    if file_path in processed_files:\n",
    "        return {\"processed\": 0, \"file_path\": file_path, \"status\": \"已跳过(已处理)\"}\n",
    "    \n",
    "    file_info = extract_file_info(file_path)\n",
    "    if not file_info:\n",
    "        return {\"processed\": 0, \"file_path\": file_path, \"status\": \"无法解析文件名\"}\n",
    "    \n",
    "    # 添加随机延迟以减少冲突\n",
    "    time.sleep(random.uniform(0.05, 0.2))\n",
    "    \n",
    "    for retry in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # 创建线程独立的连接\n",
    "            conn = get_connection_with_retry()\n",
    "            \n",
    "            # 检查文件是否已经导入\n",
    "            cursor = conn.cursor()\n",
    "            execute_with_retry(cursor, \"SELECT COUNT(*) FROM processed_files WHERE file_path = ?\", (file_path,))\n",
    "            if cursor.fetchone()[0] > 0:\n",
    "                conn.close()\n",
    "                return {\"processed\": 0, \"file_path\": file_path, \"status\": \"已跳过(已存在)\"}\n",
    "            \n",
    "            # 读取CSV文件\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "            except Exception as e:\n",
    "                conn.close()\n",
    "                return {\"processed\": 0, \"file_path\": file_path, \"status\": f\"读取CSV失败: {str(e)}\"}\n",
    "            \n",
    "            # 获取或创建交易对ID\n",
    "            symbol_id = get_or_create_symbol_id(\n",
    "                conn, \n",
    "                file_info['symbol'], \n",
    "                file_info['quote_asset'], \n",
    "                file_info['timeframe']\n",
    "            )\n",
    "            \n",
    "            # 准备数据插入\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # 创建数据元组列表用于批量插入\n",
    "            data_to_insert = []\n",
    "            for _, row in df.iterrows():\n",
    "                data_to_insert.append((\n",
    "                    symbol_id,\n",
    "                    row['datetime'],\n",
    "                    row['open'],\n",
    "                    row['high'],\n",
    "                    row['low'],\n",
    "                    row['close'],\n",
    "                    row['volume'],\n",
    "                    file_path  # 存储文件路径以便去重\n",
    "                ))\n",
    "            \n",
    "            # 开始批量事务\n",
    "            execute_with_retry(cursor, 'BEGIN TRANSACTION')\n",
    "            \n",
    "            # 批量插入数据\n",
    "            executemany_with_retry(cursor,\n",
    "                'INSERT INTO price_data (symbol_id, datetime, open, high, low, close, volume, file_path) VALUES (?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "                data_to_insert\n",
    "            )\n",
    "            \n",
    "            # 标记文件为已处理\n",
    "            mark_file_as_processed(conn, file_path)\n",
    "            \n",
    "            # 提交事务\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            \n",
    "            # 插入后短暂延迟，让其他线程有机会访问数据库\n",
    "            time.sleep(random.uniform(0.1, 0.3))\n",
    "            \n",
    "            # 显示定期详细进度\n",
    "            if file_index % 10 == 0:\n",
    "                progress = (file_index / total_files) * 100\n",
    "                print(f\"\\r当前进度: {progress:.2f}% - 正在处理: {file_path}\", end=\"\")\n",
    "            \n",
    "            return {\"processed\": len(data_to_insert), \"file_path\": file_path, \"status\": \"成功\"}\n",
    "            \n",
    "        except sqlite3.OperationalError as e:\n",
    "            if 'conn' in locals():\n",
    "                try:\n",
    "                    conn.rollback()\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            if \"database is locked\" in str(e) and retry < MAX_RETRIES - 1:\n",
    "                delay = RETRY_DELAY * (2 ** retry) + random.uniform(0, 1)\n",
    "                print(f\"处理文件 {file_path} 时数据库锁定，{delay:.2f}秒后重试... ({retry+1}/{MAX_RETRIES})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                return {\"processed\": 0, \"file_path\": file_path, \"status\": f\"处理失败: {str(e)}\"}\n",
    "        except Exception as e:\n",
    "            if 'conn' in locals():\n",
    "                try:\n",
    "                    conn.rollback()\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "            return {\"processed\": 0, \"file_path\": file_path, \"status\": f\"处理失败: {str(e)}\"}\n",
    "    \n",
    "    return {\"processed\": 0, \"file_path\": file_path, \"status\": \"处理失败: 超过最大重试次数\"}\n",
    "\n",
    "def get_all_csv_files():\n",
    "    \"\"\"获取所有CSV文件路径\"\"\"\n",
    "    all_files = []\n",
    "    for year_folder in glob.glob(os.path.join(DATA_ROOT, \"*\")):\n",
    "        if os.path.isdir(year_folder):\n",
    "            pattern = os.path.join(year_folder, \"*.csv\")\n",
    "            files = glob.glob(pattern)\n",
    "            all_files.extend(files)\n",
    "    return all_files\n",
    "\n",
    "def process_batch(batch_files, batch_num, total_batches, total_files):\n",
    "    \"\"\"处理一批文件\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 每个批次的进度条\n",
    "    batch_size = len(batch_files)\n",
    "    with tqdm(total=batch_size, desc=f\"批次 {batch_num}/{total_batches}\", unit=\"文件\") as pbar:\n",
    "        for i, file_path in enumerate(batch_files):\n",
    "            if is_interrupted:\n",
    "                break\n",
    "                \n",
    "            result = process_csv_file(file_path, i, batch_size)\n",
    "            results.append(result)\n",
    "            \n",
    "            # 更新进度条\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(文件=os.path.basename(file_path)[:20], 状态=result[\"status\"])\n",
    "    \n",
    "    # 返回批次统计信息\n",
    "    successful = sum(1 for r in results if r[\"status\"] == \"成功\")\n",
    "    skipped = sum(1 for r in results if \"已跳过\" in r[\"status\"])\n",
    "    failed = sum(1 for r in results if \"失败\" in r[\"status\"])\n",
    "    \n",
    "    return {\n",
    "        \"processed_records\": sum(r[\"processed\"] for r in results),\n",
    "        \"successful_files\": successful,\n",
    "        \"skipped_files\": skipped,\n",
    "        \"failed_files\": failed,\n",
    "        \"interrupted\": is_interrupted\n",
    "    }\n",
    "\n",
    "def import_data_parallel():\n",
    "    \"\"\"使用多线程并行导入数据\"\"\"\n",
    "    global is_interrupted\n",
    "    \n",
    "    # 创建数据库结构\n",
    "    create_database()\n",
    "    \n",
    "    # 加载已处理文件记录\n",
    "    load_processed_files()\n",
    "    \n",
    "    # 获取所有CSV文件\n",
    "    all_csv_files = get_all_csv_files()\n",
    "    total_files = len(all_csv_files)\n",
    "    \n",
    "    # 过滤掉已处理的文件\n",
    "    files_to_process = [f for f in all_csv_files if f not in processed_files]\n",
    "    \n",
    "    print(f\"找到 {total_files} 个CSV文件，其中 {len(processed_files)} 个已处理，需要处理 {len(files_to_process)} 个\")\n",
    "    \n",
    "    if len(files_to_process) == 0:\n",
    "        print(\"所有文件已处理完毕！\")\n",
    "        return\n",
    "    \n",
    "    # 记录开始时间\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # 将文件分成批次\n",
    "    batches = [files_to_process[i:i + BATCH_SIZE] for i in range(0, len(files_to_process), BATCH_SIZE)]\n",
    "    total_batches = len(batches)\n",
    "    \n",
    "    print(f\"将处理 {len(files_to_process)} 个文件，分成 {total_batches} 个批次，每批 {BATCH_SIZE} 个文件\")\n",
    "    print(f\"使用 {MAX_WORKERS} 个工作线程处理数据\")\n",
    "    \n",
    "    # 批次级别的统计\n",
    "    total_processed = 0\n",
    "    total_successful = 0\n",
    "    total_skipped = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    # 处理每个批次\n",
    "    for batch_num, batch_files in enumerate(batches, 1):\n",
    "        if is_interrupted:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\n开始处理批次 {batch_num}/{total_batches}...\")\n",
    "        \n",
    "        # 处理当前批次\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = []\n",
    "            for i, file_path in enumerate(batch_files):\n",
    "                file_idx = (batch_num - 1) * BATCH_SIZE + i\n",
    "                futures.append(executor.submit(process_csv_file, file_path, file_idx, len(files_to_process)))\n",
    "            \n",
    "            # 收集每个文件的结果\n",
    "            batch_results = []\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"批次 {batch_num}/{total_batches}\", unit=\"文件\"):\n",
    "                result = future.result()\n",
    "                batch_results.append(result)\n",
    "                \n",
    "                # 在进度条中显示最新处理的文件\n",
    "                tqdm.write(f\"已处理: {os.path.basename(result['file_path'])} - {result['status']}\")\n",
    "                \n",
    "                if is_interrupted:\n",
    "                    break\n",
    "        \n",
    "        # 计算批次统计信息\n",
    "        successful = sum(1 for r in batch_results if r[\"status\"] == \"成功\")\n",
    "        skipped = sum(1 for r in batch_results if \"已跳过\" in r[\"status\"])\n",
    "        failed = len(batch_results) - successful - skipped\n",
    "        processed_records = sum(r[\"processed\"] for r in batch_results)\n",
    "        \n",
    "        # 更新总统计\n",
    "        total_processed += processed_records\n",
    "        total_successful += successful\n",
    "        total_skipped += skipped\n",
    "        total_failed += failed\n",
    "        \n",
    "        # 显示批次统计\n",
    "        print(f\"\\n批次 {batch_num}/{total_batches} 统计:\")\n",
    "        print(f\"- 成功处理: {successful} 个文件\")\n",
    "        print(f\"- 跳过的文件: {skipped} 个文件\")\n",
    "        print(f\"- 失败的文件: {failed} 个文件\")\n",
    "        print(f\"- 导入记录数: {processed_records} 条\")\n",
    "        \n",
    "        # 显示总体进度\n",
    "        overall_progress = (batch_num / total_batches) * 100\n",
    "        print(f\"\\n总体进度: {overall_progress:.2f}% 已完成\")\n",
    "        \n",
    "        # 计算并显示预计剩余时间\n",
    "        elapsed = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        if batch_num > 0:\n",
    "            avg_time_per_batch = elapsed / batch_num\n",
    "            remaining_batches = total_batches - batch_num\n",
    "            eta_seconds = avg_time_per_batch * remaining_batches\n",
    "            eta = datetime.timedelta(seconds=int(eta_seconds))\n",
    "            print(f\"预计剩余时间: {eta}\")\n",
    "        \n",
    "        # 批次间暂停一下，减少数据库压力\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # 计算总耗时\n",
    "    end_time = datetime.datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    # 在所有数据导入完成后优化数据库\n",
    "    if not is_interrupted:\n",
    "        print(\"\\n正在优化数据库...\")\n",
    "        conn = get_connection_with_retry()\n",
    "        cursor = conn.cursor()\n",
    "        execute_with_retry(cursor, 'PRAGMA optimize')\n",
    "        conn.close()\n",
    "    \n",
    "    # 显示最终统计\n",
    "    print(\"\\n导入任务完成!\")\n",
    "    print(f\"总耗时: {datetime.timedelta(seconds=int(duration))}\")\n",
    "    print(f\"总导入记录: {total_processed}\")\n",
    "    print(f\"成功处理文件: {total_successful}\")\n",
    "    print(f\"跳过的文件: {total_skipped}\")\n",
    "    print(f\"失败的文件: {total_failed}\")\n",
    "    \n",
    "    if is_interrupted:\n",
    "        print(\"\\n注意: 程序被中断，未完成所有文件的处理。\")\n",
    "        print(\"您可以随时重新运行程序继续处理剩余文件。\")\n",
    "\n",
    "def query_data_example():\n",
    "    \"\"\"示例查询函数\"\"\"\n",
    "    conn = get_connection_with_retry()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # 创建一个便于查询的视图\n",
    "    execute_with_retry(cursor, '''\n",
    "    CREATE VIEW IF NOT EXISTS vw_price_data AS\n",
    "    SELECT \n",
    "        s.symbol, \n",
    "        s.quote_asset, \n",
    "        s.timeframe, \n",
    "        p.datetime, \n",
    "        p.open, \n",
    "        p.high, \n",
    "        p.low, \n",
    "        p.close, \n",
    "        p.volume\n",
    "    FROM price_data p\n",
    "    JOIN symbols s ON p.symbol_id = s.id\n",
    "    ''')\n",
    "    \n",
    "    # 示例1: 查询特定交易对在特定日期的数据\n",
    "    print(\"\\n示例查询1: 特定交易对在特定日期的数据\")\n",
    "    query1 = '''\n",
    "    SELECT datetime, open, high, low, close, volume\n",
    "    FROM vw_price_data\n",
    "    WHERE symbol = '1000PEPEUSDT' AND quote_asset = 'USDT' AND timeframe = '1m'\n",
    "    AND datetime LIKE '2024-01-29%'\n",
    "    LIMIT 5\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        execute_with_retry(cursor, query1)\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "        df1 = pd.DataFrame(rows, columns=column_names)\n",
    "        print(df1)\n",
    "    except Exception as e:\n",
    "        print(f\"查询1执行失败: {str(e)}\")\n",
    "    \n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 导入所有数据\n",
    "        import_data_parallel()\n",
    "        \n",
    "        # 如果没有中断，运行示例查询\n",
    "        if not is_interrupted:\n",
    "            query_data_example()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"程序执行出错: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
